<h3>Evaluation</h3>
<p>Senmaking task consists of 3 subtasks. Participating teams should participate in at-least one of the subtasks.
    Relevant scripts and datasets are available at: <a
        href="https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation">Github</a></p>
<p>Task A and B are evaluated by&nbsp;<em>accuracy</em>&nbsp;and Task C is evaluated using&nbsp;<em>BLEU</em>. To
    improve the reliability of the evaluation of Task C, we use a random subset of the test set and will do a human
    evaluation to further evaluate the systems with relatively high BLEU score.</p>