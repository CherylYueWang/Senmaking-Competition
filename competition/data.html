<h3>Dataset and Resources</h3>
<p>We ask data annotators to write instances by themselves. And we also provide them some with sources to stimulate
    inspiration, such as the raw English sentences of ConceptNet5.5 (Speer et al., 2017). For example, &ldquo;he was
    sent to a (restaurant)/(hospital) for treatment after a car crash&rdquo; were inspired by the two sentences
    &ldquo;restaurants provide food&rdquo; and &ldquo;hospitals provide medical care&rdquo; However, those corpora may
    have incorrect and one-sided knowledge, we do not use those sentences. Besides, we also let them get inspiration
    from existing commonsense reasoning questions like WSC (Levesque et al., 2012; Morgenstern and Ortiz, 2015), COPA
    (Roemmele et al., 2011) and SQUABU (Davis, 2016). Those existing corpora will never be used directly but only for
    inspiration, which means those sentences will not appear in our testset directly.</p>
<h3>Quality Control</h3>
<p>First, researchers will examine each instance case by case. Then, there is also Human Evaluation. Each instance is
    answered by at least three testees. If more than half testees do one instance wrong (either Validation or
    Explanation), we will rewrite or abolish the instance. For subtask 3, if one answer is considered unsuitable by more
    than half testees, we will rewrite it.</p>
<p>Datasets are available <a
        href="https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation">here</a></p>